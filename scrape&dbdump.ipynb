{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This file is intended to be ran only once for the initial setup and loading of raw data\r\n",
    "- Datasource: https://wildfire.alberta.ca/resources/historical-data/historical-wildfire-database.aspx"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# First, scrape the web\r\n",
    "# Import Splinter, BeautifulSoup, and Pandas\r\n",
    "from splinter import Browser\r\n",
    "from bs4 import BeautifulSoup as soup\r\n",
    "import pandas as pd\r\n",
    "from webdriver_manager.chrome import ChromeDriverManager\r\n",
    "from config import dbpassword\r\n",
    "from config import awspassword\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Driver [C:\\Users\\Steven\\.wdm\\drivers\\chromedriver\\win32\\93.0.4577.63\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Set the executable path and initialize Splinter\r\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\r\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Driver [C:\\Users\\Steven\\.wdm\\drivers\\chromedriver\\win32\\93.0.4577.63\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Visit the site\r\n",
    "url = 'https://wildfire.alberta.ca/resources/historical-data/historical-wildfire-database.aspx'\r\n",
    "browser.visit(url)\r\n",
    "\r\n",
    "# Optional delay for loading the page\r\n",
    "\r\n",
    "browser.is_element_present_by_css('div.list_text', wait_time=1)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Convert the browser html to a soup object and then quit the browser\r\n",
    "html = browser.html\r\n",
    "data = soup(html, 'html.parser')\r\n",
    "\r\n",
    "slide_elem = data.select_one('div.table-responsive')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Obtain all (relative) links that end with .csv extension and put it into a dataframe\r\n",
    "links_df =pd.DataFrame(columns=['RelLink'])\r\n",
    "for link in slide_elem.find_all('a'):\r\n",
    "    if link.get('href')[-3:] == 'csv':\r\n",
    "        links_df = links_df.append({'RelLink': link.get('href')},ignore_index=True)\r\n",
    "links_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RelLink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>documents/fires_2006to2018.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>documents/fires_1996to2005.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>documents/fires_1983to1995.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>documents/fires_1961to1982.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          RelLink\n",
       "0  documents/fires_2006to2018.csv\n",
       "1  documents/fires_1996to2005.csv\n",
       "2  documents/fires_1983to1995.csv\n",
       "3  documents/fires_1961to1982.csv"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Create base url from each relative url and name-to-be for each database table\r\n",
    "links_df['BaseLink']=[f'https://wildfire.alberta.ca/resources/historical-data/{link}' for link in links_df['RelLink']]\r\n",
    "links_df['FileName'] = [link[10:len(link)-4:1] for link in links_df['RelLink']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Final Check\r\n",
    "links_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RelLink</th>\n",
       "      <th>BaseLink</th>\n",
       "      <th>FileName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>documents/fires_2006to2018.csv</td>\n",
       "      <td>https://wildfire.alberta.ca/resources/historic...</td>\n",
       "      <td>fires_2006to2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>documents/fires_1996to2005.csv</td>\n",
       "      <td>https://wildfire.alberta.ca/resources/historic...</td>\n",
       "      <td>fires_1996to2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>documents/fires_1983to1995.csv</td>\n",
       "      <td>https://wildfire.alberta.ca/resources/historic...</td>\n",
       "      <td>fires_1983to1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>documents/fires_1961to1982.csv</td>\n",
       "      <td>https://wildfire.alberta.ca/resources/historic...</td>\n",
       "      <td>fires_1961to1982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          RelLink  \\\n",
       "0  documents/fires_2006to2018.csv   \n",
       "1  documents/fires_1996to2005.csv   \n",
       "2  documents/fires_1983to1995.csv   \n",
       "3  documents/fires_1961to1982.csv   \n",
       "\n",
       "                                            BaseLink          FileName  \n",
       "0  https://wildfire.alberta.ca/resources/historic...  fires_2006to2018  \n",
       "1  https://wildfire.alberta.ca/resources/historic...  fires_1996to2005  \n",
       "2  https://wildfire.alberta.ca/resources/historic...  fires_1983to1995  \n",
       "3  https://wildfire.alberta.ca/resources/historic...  fires_1961to1982  "
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initilize Database Connection Strings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import os\r\n",
    "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\r\n",
    "# For example:\r\n",
    "# spark_version = 'spark-3.0.3'\r\n",
    "spark_version = 'spark-3.0.3'\r\n",
    "os.environ['SPARK_VERSION']=spark_version\r\n",
    "\r\n",
    "# Install Spark and Java\r\n",
    "!apt-get update\r\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\r\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\r\n",
    "!pip install -q findspark\r\n",
    "\r\n",
    "# Set Environment Variables\r\n",
    "import os\r\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\r\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\r\n",
    "\r\n",
    "# Start a SparkSession\r\n",
    "import findspark\r\n",
    "findspark.init()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop2.7.tgz'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "^C\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Unable to find py4j, your SPARK_HOME may not be configured correctly",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Users\\Steven\\anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-64776a7403a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\Steven\\anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         raise Exception(\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         )\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download the Postgres driver that will allow Spark to interact with Postgres.\r\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "spark = SparkSession.builder.appName(\"LMPT-Forest-Fires\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "connection_string = 'lmpt-finalproject.coke2w4vs8wf.us-east-2.rds.amazonaws.com'\r\n",
    "password = dbpassword #obtain this later from an add\r\n",
    "database_name = 'postgres'\r\n",
    "\r\n",
    "# Configure settings for RDS\r\n",
    "mode = \"append\"\r\n",
    "jdbc_url=f\"jdbc:postgresql://{connection_string}:5432/{database_name}\"\r\n",
    "config = {\"user\":\"postgres\", \r\n",
    "          \"password\": password, \r\n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine both Database and Webscrape Elements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import CSVs into Datatbase via a For Loop\r\n",
    "from pyspark import SparkFiles\r\n",
    "for i,j in zip(links_df['BaseLink'],links_df['FileName']):\r\n",
    "    url=i\r\n",
    "    df = spark.read.option(\"encoding\", \"UTF-8\").csv(SparkFiles.get(\"\"), sep=\",\", header=True, inferSchema=True)\r\n",
    "    df.write.jdbc(url=jdbc_url, table=j, mode=mode, properties=config)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ca7f64627acee83618d750f45055d656a21cdf897762d86dfd25033698eb9c35"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}