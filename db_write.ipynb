{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "db_write.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NamanNYC/Final_Project/blob/database4/db_write.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eib1hLDFb9v"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc4lz9_cFUma",
        "outputId": "bf6eafb1-1d4a-41bd-9b89-5c18cb06a331"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 50.4 kB/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 88.7 kB/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to ppa.launch\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [67.9 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,428 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [581 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,335 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,770 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,202 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [613 kB]\n",
            "Fetched 13.0 MB in 4s (3,536 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpoD7oyUFeUy",
        "outputId": "c4c056ee-90f0-47b7-86f2-757117ddb59d"
      },
      "source": [
        "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-25 17:57:57--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  5.69MB/s    in 0.2s    \n",
            "\n",
            "2021-09-25 17:57:58 (5.69 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWgRSS5HFfUI"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import substring, length, col, expr, to_timestamp, date_format, round\n",
        "\n",
        "spark = SparkSession.builder.appName(\"LMPT-Forest-Fires\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGR88mX6Fh8t"
      },
      "source": [
        "connection_string = 'lmpt-finalproject.coke2w4vs8wf.us-east-2.rds.amazonaws.com'\n",
        "password = 'LMPTp4ssw0rd' \n",
        "database_name = 'postgres'\n",
        "\n",
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=f\"jdbc:postgresql://{connection_string}:5432/{database_name}\"\n",
        "config = {\"user\":\"postgres\", \n",
        "          \"password\": password, \n",
        "          \"driver\":\"org.postgresql.Driver\"}\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9AkzRMVFiaR"
      },
      "source": [
        "# Read in data \n",
        "df = spark.read.jdbc(jdbc_url,table='fires_2006to2018',properties=config)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPdq4_HVNl4F",
        "outputId": "aa941698-ff3c-4b87-dc59-f130ee18f2b8"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fire_number', 'string'),\n",
              " ('fire_name', 'string'),\n",
              " ('fire_year', 'int'),\n",
              " ('calendar_year', 'int'),\n",
              " ('assessment_datetime', 'string'),\n",
              " ('assessment_hectares', 'double'),\n",
              " ('current_size', 'double'),\n",
              " ('size_class', 'string'),\n",
              " ('fire_location_latitude', 'double'),\n",
              " ('fire_location_longitude', 'double'),\n",
              " ('fire_origin', 'string'),\n",
              " ('general_cause_desc', 'string'),\n",
              " ('industry_identifier_desc', 'string'),\n",
              " ('responsible_group_desc', 'string'),\n",
              " ('activity_class', 'string'),\n",
              " ('true_cause', 'string'),\n",
              " ('permit_detail_desc', 'string'),\n",
              " ('fire_start_date', 'string'),\n",
              " ('det_agent_type', 'string'),\n",
              " ('det_agent', 'string'),\n",
              " ('discovered_date', 'string'),\n",
              " ('reported_date', 'string'),\n",
              " ('start_for_fire_date', 'string'),\n",
              " ('fire_fighting_start_date', 'string'),\n",
              " ('fire_fighting_start_size', 'string'),\n",
              " ('initial_action_by', 'string'),\n",
              " ('fire_type', 'string'),\n",
              " ('fire_position_on_slope', 'string'),\n",
              " ('weather_conditions_over_fire', 'string'),\n",
              " ('fuel_type', 'string'),\n",
              " ('other_fuel_type', 'string'),\n",
              " ('bh_fs_date', 'string'),\n",
              " ('bh_hectares', 'double'),\n",
              " ('uc_fs_date', 'string'),\n",
              " ('uc_hectares', 'double'),\n",
              " ('to_fs_date', 'string'),\n",
              " ('to_hectares', 'string'),\n",
              " ('ex_fs_date', 'string'),\n",
              " ('ex_hectares', 'double')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsicw6tZRmz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf7c4f9-b71f-42a5-ca0e-f76be6958ae7"
      },
      "source": [
        "from pyspark.sql.functions import to_timestamp,  date_format\n",
        "df = df.withColumn('fire_start_date', to_timestamp (col('fire_start_date')))\n",
        "df = df.withColumn('assessment_datetime', to_timestamp (col('assessment_datetime')))\n",
        "df = df.withColumn('discovered_date', to_timestamp (col('discovered_date')))\n",
        "df = df.withColumn('reported_date', to_timestamp (col('reported_date')))\n",
        "df = df.withColumn('start_for_fire_date', to_timestamp (col('start_for_fire_date')))\n",
        "df = df.withColumn('fire_fighting_start_size', df['fire_fighting_start_size'].cast(\"double\"))\n",
        "df = df.withColumn('bh_fs_date', to_timestamp (col('bh_fs_date')))\n",
        "df = df.withColumn('uc_fs_date', to_timestamp (col('uc_fs_date')))\n",
        "\n",
        "\n",
        "df.dtypes\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fire_number', 'string'),\n",
              " ('fire_name', 'string'),\n",
              " ('fire_year', 'int'),\n",
              " ('calendar_year', 'int'),\n",
              " ('assessment_datetime', 'timestamp'),\n",
              " ('assessment_hectares', 'double'),\n",
              " ('current_size', 'double'),\n",
              " ('size_class', 'string'),\n",
              " ('fire_location_latitude', 'double'),\n",
              " ('fire_location_longitude', 'double'),\n",
              " ('fire_origin', 'string'),\n",
              " ('general_cause_desc', 'string'),\n",
              " ('industry_identifier_desc', 'string'),\n",
              " ('responsible_group_desc', 'string'),\n",
              " ('activity_class', 'string'),\n",
              " ('true_cause', 'string'),\n",
              " ('permit_detail_desc', 'string'),\n",
              " ('fire_start_date', 'timestamp'),\n",
              " ('det_agent_type', 'string'),\n",
              " ('det_agent', 'string'),\n",
              " ('discovered_date', 'timestamp'),\n",
              " ('reported_date', 'timestamp'),\n",
              " ('start_for_fire_date', 'timestamp'),\n",
              " ('fire_fighting_start_date', 'string'),\n",
              " ('fire_fighting_start_size', 'double'),\n",
              " ('initial_action_by', 'string'),\n",
              " ('fire_type', 'string'),\n",
              " ('fire_position_on_slope', 'string'),\n",
              " ('weather_conditions_over_fire', 'string'),\n",
              " ('fuel_type', 'string'),\n",
              " ('other_fuel_type', 'string'),\n",
              " ('bh_fs_date', 'timestamp'),\n",
              " ('bh_hectares', 'double'),\n",
              " ('uc_fs_date', 'timestamp'),\n",
              " ('uc_hectares', 'double'),\n",
              " ('to_fs_date', 'string'),\n",
              " ('to_hectares', 'string'),\n",
              " ('ex_fs_date', 'string'),\n",
              " ('ex_hectares', 'double')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b1xKTo0GPeh"
      },
      "source": [
        "detection = df[['fire_number','industry_identifier_desc','det_agent_Type','det_agent','discovered_date','reported_date']] \n",
        "loc = df[['fire_number','fire_location_latitude','fire_location_longitude']]\n",
        "cause = df[['fire_number','responsible_group_desc','activity_class','true_cause','permit_detail_desc','fire_start_date']]\n",
        "response = df[['fire_number','start_for_fire_date','fire_fighting_start_size','initial_action_by','bh_fs_date','bh_hectares','uc_fs_date','uc_hectares']]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMkHKGidF94K"
      },
      "source": [
        "detection.write.jdbc(url=jdbc_url, table='detection', mode=mode, properties=config)\n",
        "loc.write.jdbc(url=jdbc_url, table='loc', mode=mode, properties=config)\n",
        "cause.write.jdbc(url=jdbc_url, table='cause', mode=mode, properties=config)\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz_9en2gti7-"
      },
      "source": [
        "response.write.jdbc(url=jdbc_url, table='response', mode=mode, properties=config)\n"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}